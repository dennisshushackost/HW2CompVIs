{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Segmantic Segmentation with ConvNets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8028826399027550667\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 22319988736\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 900480051525402473\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# All necessary imports for the project:\n",
    "import os\n",
    "import numpy as np\n",
    "import skimage\n",
    "import skimage.data\n",
    "import skimage.io\n",
    "from pathlib import Path\n",
    "import albumentations as A\n",
    "\n",
    "# Matplotlib for visualization\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "\n",
    "# TensorFlow and parameters:\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: This part shows the entire code for the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Preprocessor class in the provided code is designed to handle the pre-processing of image data for the convolutional networks. The processed tensorflow datasets are saved after the execution. It performs:\n",
    "- Min-Max normalization of the image data\n",
    "- One-hot encoding of the labels\n",
    "- Data shuffling, caching, batching and the creation of tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(): \n",
    "    \"\"\"\n",
    "    This class is in charge of pre-processing the data and making \n",
    "    it suitable for the model.\n",
    "    path: path to the directory containing the images and labels\n",
    "    batch_size: batch size for training \n",
    "    num_classes: number of classes in the dataset (34 for this project)\n",
    "    patch_size_width: width of the patches (256)\n",
    "    patch_size_height: height of the patches (256)\n",
    "    file_type_str: 'train', 'test', or 'validate'\n",
    "    border: border size\n",
    "    channels: number of channels (3 for RGB)     \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 path,\n",
    "                 batch_size,\n",
    "                 num_classes, \n",
    "                 patch_size_width, \n",
    "                 patch_size_height,\n",
    "                 file_type_str,\n",
    "                 border=0, \n",
    "                 channels=3\n",
    "                 ):\n",
    "        \n",
    "        self.path = path + file_type_str + '/'\n",
    "        self.batch_size = batch_size\n",
    "        self.border = border\n",
    "        self.num_classes = num_classes\n",
    "        self.patch_size_width = patch_size_width\n",
    "        self.patch_size_height = patch_size_height\n",
    "        self.file_type_str = file_type_str\n",
    "        self.channels = channels\n",
    "        self.dataset = None\n",
    "\n",
    "        print(f\"Preprocessing the {self.file_type_str} data...\")\n",
    "\n",
    "    def create_dataset(self):\n",
    "        # Get a list of the files:\n",
    "        path = Path(self.path)\n",
    "        image_files = list(path.glob(self.file_type_str+'_img*.png'))\n",
    "        label_files = list(path.glob(self.file_type_str+'_lbl*.png'))\n",
    "\n",
    "        # Sort the files: \n",
    "        image_files.sort()\n",
    "        label_files.sort()\n",
    "\n",
    "        # Check if we have an equal number of images and labels:\n",
    "        if len(image_files) != len(label_files):\n",
    "            raise ValueError('The number of images and labels does not match.')\n",
    "\n",
    "        # Read the images and label file_directories into a numpy array:\n",
    "        image_files_array = np.asarray([str(p) for p in image_files])\n",
    "        label_files_array = np.asarray([str(p) for p in label_files])\n",
    "      \n",
    "        # Create a dataset from the file_directories:\n",
    "        self.dataset = tf.data.Dataset.from_tensor_slices((image_files_array, label_files_array))\n",
    "        self.dataset = self.dataset.shuffle(buffer_size=10000)\n",
    "\n",
    "        self.dataset = self.dataset.map(lambda image, file:\n",
    "                        self._parse_function(image, file))\n",
    "\n",
    "        # Parse the images and labels to the correct form:\n",
    "        self.dataset = self.dataset.map(lambda x, y:\n",
    "                        (tf.reshape(x, shape=(self.patch_size_width, self.patch_size_height, 3)),\n",
    "                        tf.reshape(y, shape=(self.patch_size_width, self.patch_size_height))))\n",
    "        \n",
    "        # cut center of the label image in order to use valid filtering in the network\n",
    "        b = self.border\n",
    "        if b != 0:\n",
    "            self.dataset = self.dataset.map(lambda x, y:\n",
    "                                    (x, y[b:-b, b:-b]))\n",
    "        \n",
    "        # We one-hot encode the labels:\n",
    "        self.dataset = self.dataset.map(lambda x, y: (x, tf.one_hot(y, depth=self.num_classes, dtype=tf.float32)))\n",
    "\n",
    "        print(\"Done preprocessing the data.\")\n",
    "\n",
    "        return self.dataset, image_files_array.size\n",
    "\n",
    "\n",
    "    def _parse_function(self, image_filename, label_filename):\n",
    "        \"\"\"\n",
    "        This function reads the image and label files and returns the\n",
    "        image and label tensors.\n",
    "        \"\"\"\n",
    "\n",
    "        # Read the image and label into a tensor:\n",
    "        image_string = tf.io.read_file(image_filename)\n",
    "        label_string = tf.io.read_file(label_filename)\n",
    "\n",
    "        # Decode the image and label:\n",
    "        image = tf.image.decode_png(image_string, channels=self.channels)\n",
    "        label = tf.image.decode_png(label_string, channels=1)\n",
    "\n",
    "        # Convert the image and label to float32:\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        label = tf.cast(label, tf.float32)\n",
    "\n",
    "        # Min-max normalization:\n",
    "        image_min = tf.reduce_min(image)\n",
    "        image_max = tf.reduce_max(image)\n",
    "        image = (image - image_min)/(image_max - image_min + 1e-7)\n",
    "\n",
    "        # Decode label:\n",
    "        label = tf.image.decode_png(label_string, dtype=tf.uint8, channels=1)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We define the simple FCN model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_simple_fcn(input_shape, num_classes):\n",
    "    model = models.Sequential()    \n",
    "\n",
    "\n",
    "    model.add(layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', strides=(1,1), input_shape=input_shape))\n",
    "    model.add(layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', strides=(1,1)))\n",
    "\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same', strides=(1,1)))\n",
    "    model.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same', strides=(1,1)))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Convolution2D(filters=num_classes, kernel_size=1, activation=None, padding=\"same\", strides=(1,1)))\n",
    "    model.add(layers.Activation('softmax'))\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing the train data...\n",
      "Done preprocessing the data.\n",
      "Preprocessing the test data...\n",
      "Done preprocessing the data.\n",
      "Preprocessing the val data...\n",
      "Done preprocessing the data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "\n",
    "def build_unet_like_model(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Downsampling\n",
    "    c1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "    c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    # Bottleneck\n",
    "    bn = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p2)\n",
    "\n",
    "    # Upsampling\n",
    "    u1 = layers.UpSampling2D((2, 2))(bn)\n",
    "    concat1 = layers.concatenate([u1, c2])\n",
    "    c3 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(concat1)\n",
    "    u2 = layers.UpSampling2D((2, 2))(c3)\n",
    "    concat2 = layers.concatenate([u2, c1])\n",
    "    c4 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(concat2)\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = layers.Conv2D(num_classes, (1, 1), activation='softmax')(c4)\n",
    "\n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Data paths and parameters\n",
    "DIR_PATH = './data/'\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 34  # Update with the actual number of your classes\n",
    "PATCH_SIZE_WIDTH, PATCH_SIZE_HEIGHT = 256, 256  # Adjust sizes as needed\n",
    "DIR_STR = ['train', 'test', 'val']\n",
    "BORDER = 0\n",
    "\n",
    "# Data preprocessing\n",
    "preprocessor_train = Preprocessor(DIR_PATH, BATCH_SIZE, NUM_CLASSES, PATCH_SIZE_WIDTH, PATCH_SIZE_HEIGHT, DIR_STR[0], BORDER, 3)\n",
    "dataset_train, nr_images_train = preprocessor_train.create_dataset()\n",
    "dataset_train = dataset_train.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "preprocessor_test = Preprocessor(DIR_PATH, BATCH_SIZE, NUM_CLASSES, PATCH_SIZE_WIDTH, PATCH_SIZE_HEIGHT, DIR_STR[1], BORDER, 3)\n",
    "dataset_test, nr_images_test = preprocessor_test.create_dataset()\n",
    "dataset_test = dataset_test.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "preprocessor_val = Preprocessor(DIR_PATH, BATCH_SIZE, NUM_CLASSES, PATCH_SIZE_WIDTH, PATCH_SIZE_HEIGHT, DIR_STR[2], BORDER, 3)\n",
    "dataset_val, nr_images_val = preprocessor_val.create_dataset()\n",
    "dataset_val = dataset_val.batch(BATCH_SIZE)\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (PATCH_SIZE_WIDTH, PATCH_SIZE_HEIGHT, 3)\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"your_segmentation_project\", entity=\"your_wandb_username\")\n",
    "wandb.config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"num_classes\": NUM_CLASSES,\n",
    "}\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_unet_like_model(input_shape, wandb.config.num_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    steps_per_epoch=nr_images_train // wandb.config.batch_size,\n",
    "    epochs=wandb.config.epochs,\n",
    "    validation_data=dataset_val,\n",
    "    validation_steps=nr_images_val // wandb.config.batch_size,\n",
    "    callbacks=[WandbCallback()]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(dataset_test, steps=nr_images_test // wandb.config.batch_size)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Log test results to wandb\n",
    "wandb.log({'test_loss': test_loss, 'test_accuracy': test_accuracy})\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing the train data...\n",
      "Done preprocessing the data.\n",
      "Preprocessing the test data...\n",
      "Done preprocessing the data.\n",
      "Preprocessing the val data...\n",
      "Done preprocessing the data.\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_37 (Conv2D)          (None, 256, 256, 32)      896       \n",
      "                                                                 \n",
      " conv2d_38 (Conv2D)          (None, 256, 256, 32)      9248      \n",
      "                                                                 \n",
      " conv2d_39 (Conv2D)          (None, 256, 256, 64)      18496     \n",
      "                                                                 \n",
      " conv2d_40 (Conv2D)          (None, 256, 256, 64)      36928     \n",
      "                                                                 \n",
      " conv2d_41 (Conv2D)          (None, 256, 256, 34)      2210      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 256, 256, 34)      0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67778 (264.76 KB)\n",
      "Trainable params: 67778 (264.76 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "122/122 [==============================] - 23s 179ms/step - loss: 2.1574 - accuracy: 0.3600 - val_loss: 2.3196 - val_accuracy: 0.3579\n",
      "Epoch 2/10\n",
      "122/122 [==============================] - 22s 185ms/step - loss: 1.8308 - accuracy: 0.4397 - val_loss: 2.3496 - val_accuracy: 0.3888\n",
      "Epoch 3/10\n",
      "122/122 [==============================] - 23s 186ms/step - loss: 1.7543 - accuracy: 0.4650 - val_loss: 2.3574 - val_accuracy: 0.3924\n",
      "Epoch 4/10\n",
      "122/122 [==============================] - 23s 191ms/step - loss: 1.6737 - accuracy: 0.4914 - val_loss: 2.2642 - val_accuracy: 0.4171\n",
      "Epoch 5/10\n",
      "122/122 [==============================] - 22s 183ms/step - loss: 1.6477 - accuracy: 0.4975 - val_loss: 2.2886 - val_accuracy: 0.4017\n",
      "Epoch 6/10\n",
      "122/122 [==============================] - 22s 181ms/step - loss: 1.6272 - accuracy: 0.5052 - val_loss: 2.4389 - val_accuracy: 0.3803\n",
      "Epoch 7/10\n",
      "122/122 [==============================] - 22s 177ms/step - loss: 1.6071 - accuracy: 0.5125 - val_loss: 2.2980 - val_accuracy: 0.4304\n",
      "Epoch 8/10\n",
      "122/122 [==============================] - 22s 180ms/step - loss: 1.5900 - accuracy: 0.5175 - val_loss: 2.3389 - val_accuracy: 0.4291\n",
      "Epoch 9/10\n",
      "122/122 [==============================] - 22s 183ms/step - loss: 1.5643 - accuracy: 0.5275 - val_loss: 2.2719 - val_accuracy: 0.4424\n",
      "Epoch 10/10\n",
      "122/122 [==============================] - 23s 186ms/step - loss: 1.5551 - accuracy: 0.5287 - val_loss: 2.2899 - val_accuracy: 0.4408\n",
      "59/59 [==============================] - 3s 55ms/step - loss: 2.4743 - accuracy: 0.3700\n",
      "Test Loss: 2.47434663772583, Test Accuracy: 0.3699706494808197\n"
     ]
    }
   ],
   "source": [
    "DIR_PATH = './data/'\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 34 # Update with the actual number of your classes\n",
    "PATCH_SIZE_WIDTH, PATCH_SIZE_HEIGHT = 256, 256  # Adjust sizes as needed\n",
    "DIR_STR = ['train', 'test', 'val']\n",
    "BORDER = 0\n",
    "\n",
    "\n",
    "preprocessor_train = Preprocessor(DIR_PATH, BATCH_SIZE, NUM_CLASSES, PATCH_SIZE_WIDTH, PATCH_SIZE_HEIGHT, DIR_STR[0], BORDER, 3)\n",
    "\n",
    "# Create the training dataset:\n",
    "dataset_train, nr_images_train = preprocessor_train.create_dataset()\n",
    "dataset_train = dataset_train.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "# Create the test dataset:\n",
    "preprocessor_test = Preprocessor(DIR_PATH, BATCH_SIZE, NUM_CLASSES, PATCH_SIZE_WIDTH, PATCH_SIZE_HEIGHT, DIR_STR[1], BORDER, 3)\n",
    "dataset_test, nr_images_test = preprocessor_test.create_dataset()\n",
    "dataset_test = dataset_test.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "# Create the validation dataset:\n",
    "preprocessor_val = Preprocessor(DIR_PATH, BATCH_SIZE, NUM_CLASSES, PATCH_SIZE_WIDTH, PATCH_SIZE_HEIGHT, DIR_STR[2], BORDER, 3)\n",
    "dataset_val, nr_images_val = preprocessor_val.create_dataset()\n",
    "dataset_val = dataset_val.batch(BATCH_SIZE)\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (PATCH_SIZE_WIDTH, PATCH_SIZE_HEIGHT, 3)\n",
    "\n",
    "# Build the model\n",
    "model = build_simple_fcn(input_shape, NUM_CLASSES)\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(dataset_train,\n",
    "                    steps_per_epoch=nr_images_train // BATCH_SIZE,\n",
    "                    epochs=10,  # Set the number of epochs\n",
    "                    validation_data=dataset_val,\n",
    "                    validation_steps=nr_images_val // BATCH_SIZE)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(dataset_test, steps=nr_images_test // BATCH_SIZE)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
